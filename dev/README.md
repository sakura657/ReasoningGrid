# Reasoning Model Evaluation Automation Suite

This document describes an automated suite for running reasoning model evaluations using `lighteval`, analyzing the results, and generating comprehensive reports.

## File Descriptions

- **`run_interactive.sh`**: The main orchestration script. It sets up the environment, iterates through models and parameters, runs the `lighteval` evaluations, and then triggers the post-processing analysis and conversion scripts.
- **`main.py`**: The entry point for the `lighteval` evaluation runner.
- **`lighteval_tasks.py`**: Defines the custom evaluation tasks to be used by `lighteval`. It dynamically loads prompts from `prompts.json`.
- **`prompts.json`**: A centralized JSON file to store all prompts, such as the `SYSTEM_PROMPT` and task-specific templates (`MATH_QUERY_TEMPLATE`).
- **`analyze_results.py`**: The first-pass analysis script. It scans the `lighteval` output directories, parses raw results and details (from `.json` and `.parquet` files), and aggregates them into a summary file (`all_experiments_results.json`).
- **`analyze_summary.py`**: The second-pass analysis script. It takes the aggregated results from the previous step and performs detailed statistical analysis, grouping data by seed and configuration. It calculates means, standard deviations, and other metrics, outputting a final `analysis_results.json` and two summary CSV files (`seed_analysis.csv` and `config_analysis.csv`).
- **`convert_parquet_to_csv.py`**: A utility script that finds all `.parquet` files in the output directory and converts them to `.csv` format.

## Main Parameters

The `run_interactive.sh` script calls `main.py` with the following key parameters to control the evaluation process:

- `--model`: The identifier for the Hugging Face model or the path to a local model to be evaluated.
- `--task`: The specific evaluation tasks to run.
- `--temperature`: The sampling temperature for generation.
- `--top_p`: The nucleus sampling threshold.
- `--seed`: The random seed used for reproducibility.
- `--output_dir`: The base directory where all evaluation results will be stored.
- `--max_new_tokens`: The maximum number of tokens to be generated by the model for each prompt.
- `--max_model_length`: The maximum sequence length that the model can handle.
- `--custom_tasks_directory`: The path to the Python file defining custom evaluation tasks.
- `--use_chat_template`: Set apply_chat_template and it's parameter add_generation_prompt(add the tokens that indicate the start of a bot response).
- `--system_prompt`: The system prompt to be used when `--use_chat_template` is active.
- `--dtype`: The data type for model inference (e.g., `bfloat16`, `float16`).
- `--max_num_seqs`: The maximum number of sequences to process in a single batch.
- `--max_num_batched_tokens`: The maximum total number of tokens across all sequences in a batch.
- `--tensor_parallel_size`, `--pipeline_parallel_size`, `--data_parallel_size`: Parameters for configuring distributed execution with vLLM.

## How to Run

### 1. Configuration

All configurations are done within the `run_interactive.sh` script. Open it and modify the following sections as needed:

- **Paths**:
  - `LOCAL_DIR`: Set this to the absolute path of the directory containing the scripts.
  - `OUTPUT_DIR`: Specify the directory where all evaluation outputs and analysis reports will be saved.

- **Evaluation Parameters**:
  - `MODELS`: An array of Hugging Face model identifiers or local paths to the models you want to evaluate.
  - `MAX_NUM_SEQUENCES`, `MAX_NUM_BATCHED_TOKENS`, etc.: Set hyperparameters for the evaluation runs.
  - `SEEDS`: An array of random seeds to run for each configuration, enabling statistical analysis of result variance.
  - `TASKS`: An array defining the custom tasks to run.

  **Prompts(prompts.json)**:
  - `SYSTEM_PROMPT`: Set this for system prompt.
  - `MATH_QUERY_TEMPLATE`: Set the user prompt here.


### 2. Execution

Once configured, run the script from your terminal:

```bash
bash run_interactive.sh
```

## Output Structure

All results will be saved in the `OUTPUT_DIR` you specified. The structure will look like this:

```
<OUTPUT_DIR>/
├── <model_name_with_underscores>/
│   ├── all_experiments_results.json
│   ├── analysis_results.json
│   ├── config_analysis.csv
│   ├── seed_analysis.csv
│   ├── <experiment_config_1>/
│   │   ├── details/
│   │   │   ├── ... .parquet
│   │   │   └── ... .csv
│   │   └── results/
│   │       └── ... .json
│   └── <experiment_config_2>/
│       └── ...
└── logs/
    └── ...
```